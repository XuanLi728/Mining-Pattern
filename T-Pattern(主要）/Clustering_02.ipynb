{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### 数据集\n",
    "Geolife GPS trajectory dataset\n",
    "#### idea\n",
    "聚类得出每类的代表性轨迹，而后计算每条轨迹与其的距离差与时间差（二者看看用什么函数），将距离矩阵投入 Prophet中预测\n",
    "##### TODO_List\n",
    "- [x] 聚类（时间差与距离）\n",
    "- [x] 轨迹平滑处理\n",
    "   - [x] 代码测试通过\n",
    "   - [x] 寻找最佳参数--------除了I外的个元素越大越平滑\n",
    "- [x] 代表性的轨迹\n",
    "   - [x] 聚类中心--- clusters.centroids[clustersIndex][:,0],clusters.centroids[clustersIndex][:,1]\n",
    "   - [ ] 轨迹点数目的归一化----Ramer-Douglas-Peucker Algorithm\n",
    "    - [ ] 归一化参数选择，确保都是一样数目的点\n",
    "   - [ ] 距离矩阵的计算\n",
    "   - [ ] 如何仅取ndarray的前2轴\n",
    "- [ ] Prophet 使用\n",
    "\n",
    "\n",
    "##### 算法框架\n",
    "1.  将原始轨迹按 ID 读入内存中，卡尔曼滤波后再进行计算\n",
    "2.  进行QuickBundles聚类，得出聚类中心线\n",
    "3.  每类中的所有轨迹与其中心线的距离矩阵的计算\n",
    "4.  将距离矩阵 + 具体时间点投入Prophet中，注意每个 ID 可能会对应多个预测模型，但每个类仅对应一个预测模型\n",
    "5.  调参\n",
    "\n",
    "##### 可能问题\n",
    "1.  GPS定位不精确导致的位置漂移严重与滞后\n",
    "2.  新加入轨迹需重新训练\n",
    "3.  对 ID - 模型的匹配问题\n",
    "4.  仅能预测有规律的人员出行(上班族)，对随机人群(出租车，销售人员)等无法预测\n",
    "5.  点间时间跨度可能很大，可能造成轨迹的偏差\n",
    "6.  离群曲线可能造成聚类较大的偏差\n",
    "\n",
    "##### 可能的解决方向\n",
    "1.  点聚类设置 ROI， 并就距离(点与聚类中心)来对点进行拟合(以聚类中心为代表)以减少点的数量和对轨迹进行优化\n",
    "2.  用较小的 threshold 聚类后， 删除 indices 较少的类别，再进行较高 threshold 的聚类 ( 但可能会损失精度 )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据处理（读取+kalman）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# 数据处理\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from gmplot import gmplot\n",
    "from pykalman import KalmanFilter\n",
    "# Enable inline plotting\n",
    "\n",
    "names = ['lat','lng','zero','alt','days','date','time']\n",
    "streams_work = []\n",
    "streams_weekends = []\n",
    "TimeStamp = []\n",
    "index = 0\n",
    "iter = 5\n",
    "userdata = 'data\\\\Geolife Trajectories 1.3\\\\Data\\\\' + '001' + '\\\\Trajectory\\\\'\n",
    "filelist = os.listdir(userdata)\n",
    "\n",
    "def func_kalman_2(array):\n",
    "    measurements = array\n",
    "\n",
    "    initial_state_mean = [measurements[0, 0],\n",
    "                      0,\n",
    "                      measurements[0, 1],\n",
    "                      0]\n",
    "\n",
    "    transition_matrix = [[1, 8, 0, 0],\n",
    "                         [0, 1, 0, 0],\n",
    "                         [0, 0, 1, 8],\n",
    "                         [0, 0, 0, 1]]\n",
    "\n",
    "    observation_matrix = [[1 ,0, 0, 0],\n",
    "                          [0, 0, 1, 0]]\n",
    "\n",
    "    kf1 = KalmanFilter(transition_matrices = transition_matrix,\n",
    "                  observation_matrices = observation_matrix,\n",
    "                  initial_state_mean = initial_state_mean)\n",
    "    kf1 = kf1.em(measurements, n_iter=iter)\n",
    "    (smoothed_state_means, smoothed_state_covariances) = kf1.smooth(measurements)\n",
    "    kf2 = KalmanFilter(transition_matrices = transition_matrix,\n",
    "                  observation_matrices = observation_matrix,\n",
    "                  initial_state_mean = initial_state_mean,\n",
    "                  observation_covariance = 20*kf1.observation_covariance,\n",
    "                  em_vars=['transition_covariance', 'initial_state_covariance'])\n",
    "\n",
    "    kf2 = kf2.em(measurements, n_iter=iter)\n",
    "    (smoothed_state_means, smoothed_state_covariances)  = kf2.smooth(measurements)\n",
    "    return smoothed_state_means\n",
    "\n",
    "for f in tqdm(filelist):\n",
    "    df_list = [pd.read_csv(userdata + f,header=6,names=names,index_col=False, parse_dates=[['date', 'time']])]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df.drop(['zero','alt','days'], axis=1, inplace=True)\n",
    "    df.set_index('date_time')\n",
    "    df_min = df.iloc[::12, :]    # 1min\n",
    "\n",
    "    time_h = df_min['date_time'].apply(lambda x:time.mktime(x.timetuple()))\n",
    "\n",
    "    smooth = func_kalman_2(np.c_[df_min['lat'].values, df_min['lng'].values])\n",
    "    lat_lng_data = np.c_[smooth[:,0], smooth[:,2], time_h]\n",
    "    if df_min['date_time'].dt.dayofweek.mode().values[0] < 5:\n",
    "        streams_work.append(lat_lng_data)\n",
    "    else:\n",
    "        streams_weekends.append(lat_lng_data)\n",
    "\n",
    "print(streams_work[0][0,0], streams_work[0][0,1], streams_work[0][0,2])\n",
    "\n",
    "# gmap = gmplot.GoogleMapPlotter(streams[0][0,0], streams[0][0,1], 12)\n",
    "\n",
    "# for i in range(len(streams)):\n",
    "#     gmap.plot(streams[i][:,0], streams[i][:,1], 'cornflowerblue', edge_width=1)\n",
    "\n",
    "# gmap.draw(\"user001_map.html\")\n",
    "# print('done')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 分类 (QuickBundles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 分类 1\n",
    "import geopy.distance\n",
    "from dipy.segment.metric import Metric\n",
    "from dipy.segment.metric import ResampleFeature\n",
    "import numpy as np\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "\n",
    "STREAMS = streams_weekends\n",
    "\n",
    "THRESHOLD = 1\n",
    "\n",
    "class GPSDistance(Metric):\n",
    "    def __init__(self):\n",
    "        super(GPSDistance, self).__init__(feature=ResampleFeature(nb_points=300))\n",
    "\n",
    "    def are_compatible(self, shape1, shape2):\n",
    "        return len(shape1) == len(shape2)\n",
    "\n",
    "    def dist(self, v1, v2):\n",
    "        x = [geopy.distance.distance([p[0][0], p[0][1]], [p[1][0], p[1][1]]).kilometers  for p in list(zip(v1, v2)) if p[0][2] - p[1][2] < 10]\n",
    "        currD = np.mean(x)\n",
    "        return currD\n",
    "\n",
    "metric = GPSDistance()\n",
    "qb = QuickBundles(threshold=THRESHOLD, metric=metric)\n",
    "print (time.strftime('%H:%M:%S',time.localtime(time.time())))\n",
    "clusters = qb.cluster(STREAMS)\n",
    "print (time.strftime('%H:%M:%S',time.localtime(time.time())))\n",
    "print(\"Nb. clusters:\", len(clusters))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 分类 - 2\n",
    "# 调整 threshold 去除仅含1条轨迹的类别\n",
    "# 双 聚类 代码\n",
    "import geopy.distance\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "\n",
    "from dipy.segment.metric import Metric\n",
    "from dipy.segment.metric import ResampleFeature\n",
    "from tqdm import tqdm\n",
    "\n",
    "STREAMS = streams_work\n",
    "\n",
    "Min_Trajectory_Number = 1\n",
    "\n",
    "# 第一次聚类（低threshold 聚多类, 除去离群轨迹)\n",
    "THRESHOLD_1 = 0.7\n",
    "\n",
    "class GPSDistance(Metric):\n",
    "    def __init__(self):\n",
    "        super(GPSDistance, self).__init__(feature=ResampleFeature(nb_points=300))\n",
    "\n",
    "    def are_compatible(self, shape1, shape2):\n",
    "        return len(shape1) == len(shape2)\n",
    "\n",
    "    def dist(self, v1, v2):\n",
    "        x = [geopy.distance.distance([p[0][0], p[0][1]], [p[1][0], p[1][1]]).kilometers  for p in list(zip(v1, v2)) if p[0][2] - p[1][2] < 10]\n",
    "        currD = np.mean(x)\n",
    "        return currD\n",
    "\n",
    "metric = GPSDistance()\n",
    "qb = QuickBundles(threshold=THRESHOLD_1, metric=metric)\n",
    "\n",
    "clusters = qb.cluster(STREAMS)\n",
    "print(\"First Nb. clusters:\", len(clusters))\n",
    "\n",
    "# 去除离群轨迹\n",
    "Keep_streams = []\n",
    "mask = clusters > Min_Trajectory_Number\n",
    "for index in tqdm(range(len(clusters))):\n",
    "    if mask[index]:\n",
    "        for i in clusters[index].indices:\n",
    "            Keep_streams.append(STREAMS[i])\n",
    "\n",
    "\n",
    "THRESHOLD_2 = 1.3\n",
    "qb = QuickBundles(threshold=THRESHOLD_2, metric=metric)\n",
    "\n",
    "clusters_2 = qb.cluster(Keep_streams)\n",
    "print(\"Second Nb. clusters:\", len(clusters_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 画图（gmplot）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 画图 -- 1\n",
    "from gmplot import gmplot\n",
    "import random\n",
    "\n",
    "CLUSTER = clusters\n",
    "\n",
    "def randomcolor():\n",
    "    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']\n",
    "    color = \"\"\n",
    "    for i in range(6):\n",
    "        color += colorArr[random.randint(0,14)]\n",
    "    return \"#\"+color\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(streams_work[0][0,0], streams_work[0][0,1], 12)\n",
    "\n",
    "for clustersIndex in range(len(CLUSTER)):\n",
    "    color = randomcolor()\n",
    "    for i in CLUSTER[clustersIndex].indices:\n",
    "        gmap.plot(streams_work[i][:,0], streams_work[i][:,1], color, edge_width=1)\n",
    "\n",
    "gmap.draw(\"user001_map.html\")\n",
    "print('Map done')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 画图 -- 2\n",
    "\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['figure.facecolor']=(1,1,1,1)\n",
    "# i = 0\n",
    "clusters_sample = clusters_2\n",
    "for i in range(len(clusters_sample)):\n",
    "#     plt.plot(clusters_2.centroids[i][:,0], clusters_2.centroids[i][:,1], label='centroid')\n",
    "    for j in clusters_sample[i].indices:\n",
    "        plt.plot(streams_work[j][:,0], streams_work[j][:,1])\n",
    "\n",
    "# for i in range(len(clusters_2)):\n",
    "#     plt.plot(clusters_2.centroids[i][:,0], clusters_2.centroids[i][:,1], label='centroid')\n",
    "#     for j in clusters_2[i].indices:\n",
    "#         plt.plot(streams_work[j][:,0], streams_work[j][:,1]) # Plot some data on the (implicit) axes.\n",
    "\n",
    "# plt.xlabel('latitude')\n",
    "# plt.ylabel('longitude')\n",
    "# plt.title(\"Simple Plot\")\n",
    "# plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def center_change_time_type(centroids):\n",
    "\n",
    "    Test_Array = centroids\n",
    "\n",
    "    dt1 = pd.date_range(start='2020-01-01',end='2020-01-02',freq=\"4.8T\") # 300个点，与上文的Resample一致, 时间随意，因为不会用到\n",
    "    pydate_array = dt1.to_pydatetime()\n",
    "    pydate_array = np.delete(pydate_array,-1)\n",
    "    Test_Array = np.delete(Test_Array, -1, axis=1)\n",
    "    Test_Array = np.c_[Test_Array, pydate_array]\n",
    "\n",
    "    return Test_Array\n",
    "\n",
    "def stream_change_time_type(stream):\n",
    "\n",
    "    temp_var = np.vectorize(lambda s: datetime.fromtimestamp(time.mktime(time.localtime(s))))(stream[:,2])\n",
    "    temp_array = np.delete(stream, -1, axis=1)\n",
    "    stream = np.c_[temp_array, temp_var]\n",
    "\n",
    "    return stream\n",
    "\n",
    "def find_nearest_time_point(centroids, stream):\n",
    "\n",
    "    index_of_point = np.argmin(np.vectorize(lambda t, s: (t-s).seconds)(center_change_time_type(centroids)[:,2], stream_change_time_type(stream)[0,2]))\n",
    "\n",
    "    if centroids.shape[0] - index_of_point >= stream.shape[0]:\n",
    "        is_longer_than_stream = True\n",
    "    else:\n",
    "        is_longer_than_stream = False\n",
    "\n",
    "    return index_of_point, is_longer_than_stream\n",
    "\n",
    "print(clusters_2.centroids[0].shape)\n",
    "print(streams_work[0].shape)\n",
    "print(np.vectorize(lambda x:time.strftime('%Y/%m/%d %H:%M:%S',time.localtime(x)))(streams_work[0][0,2]))\n",
    "print(clusters_2.centroids[0])\n",
    "index_of_point , is_longer_than_stream = find_nearest_time_point(clusters_2.centroids[0], streams_work[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "distance_list = []\n",
    "time_list = []\n",
    "def center_change_time_type(centroids):\n",
    "\n",
    "    Test_Array = centroids\n",
    "\n",
    "    dt1 = pd.date_range(start='2020-01-01',end='2020-01-02',freq=\"4.8T\") # 300个点，与上文的Resample一致, 时间随意，因为不会用到\n",
    "    pydate_array = dt1.to_pydatetime()\n",
    "    pydate_array = np.delete(pydate_array,-1)\n",
    "    Test_Array = np.delete(Test_Array, -1, axis=1)\n",
    "    Test_Array = np.c_[Test_Array, pydate_array]\n",
    "\n",
    "    return Test_Array\n",
    "\n",
    "def stream_change_time_type(stream):\n",
    "\n",
    "    temp_var = np.vectorize(lambda s: datetime.fromtimestamp(time.mktime(time.localtime(s))))(stream[:,2])\n",
    "    temp_array = np.delete(stream, -1, axis=1)\n",
    "    stream = np.c_[temp_array, temp_var]\n",
    "\n",
    "    return stream\n",
    "\n",
    "def find_nearest_time_point(centroids, stream):\n",
    "\n",
    "    index_of_point = np.argmin(np.vectorize(lambda t, s: (t-s).seconds)(center_change_time_type(centroids)[:,2], stream_change_time_type(stream)[0,2]))\n",
    "\n",
    "    if centroids.shape[0] - index_of_point >= stream.shape[0]:\n",
    "        is_longer_than_stream = True\n",
    "    else:\n",
    "        is_longer_than_stream = False\n",
    "\n",
    "    return index_of_point, is_longer_than_stream\n",
    "\n",
    "for i in tqdm(clusters_2[3].indices):\n",
    "\n",
    "    index_of_point , is_longer_than_stream = find_nearest_time_point(clusters_2.centroids[0], streams_work[i])\n",
    "\n",
    "    v1 = clusters_2.centroids[0][index_of_point:,:]\n",
    "\n",
    "    if is_longer_than_stream:\n",
    "        v2 = streams_work[i]\n",
    "    else:\n",
    "        v2 = streams_work[i][:v1.shape[0],:]\n",
    "\n",
    "    x = [geopy.distance.distance([p[0][0], p[0][1]], [p[1][0], p[1][1]]).kilometers for p in list(zip(v1, v2))]\n",
    "    # x = np.c_[x,v2[:,2]]\n",
    "    distance_list = distance_list + x\n",
    "    time_list = time_list + np.vectorize(lambda x:time.strftime('%H:%M:%S',time.localtime(x)))(v2[:,2]).tolist()\n",
    "\n",
    "print(len(distance_list), len(time_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "print(time_list[0] , time_list[300] , time_list[430])\n",
    "rcParams['figure.facecolor']=(1,1,1,1)\n",
    "x = time_list\n",
    "y = distance_list\n",
    "plt.scatter(x, y, alpha=0.6)  # 绘制散点图，透明度为0.6（这样颜色浅一点，比较好看）\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "mm=pd.DataFrame(time_list,columns=['ds'])\n",
    "mm=pd.concat([mm,pd.DataFrame(distance_list,columns=['y'])],axis=1)\n",
    "\n",
    "x = mm['y']\n",
    "y = mm['ds']\n",
    "plt.scatter(x, y, alpha=0.6)  # 绘制散点图，透明度为0.6（这样颜色浅一点，比较好看）\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "# df = pd.read_csv('data\\example_wp_log_peyton_manning.csv')\n",
    "# df.head()\n",
    "df = mm\n",
    "m = Prophet()\n",
    "m.fit(df)\n",
    "\n",
    "future = m.make_future_dataframe(periods=1, freq='H')\n",
    "fcst = m.predict(future)\n",
    "print(fcst[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head())\n",
    "\n",
    "fig = m.plot(fcst)\n",
    "fig = m.plot_components(fcst)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "#\n",
    "#\n",
    "# def find_nearest(centroids, stream):\n",
    "#     array = np.asarray(centroids)\n",
    "#     Longer_array = -1 # 1 -> left longer , 0 -> right longer\n",
    "#     if array.shape[0] >= stream.shape[0]:\n",
    "#         idx = (np.abs(array[:,2] - stream[0,2])).argmin()\n",
    "#         Longer_array = 1\n",
    "#     else:\n",
    "#         idx = (np.abs(array[0,2] - stream[:,2])).argmin()\n",
    "#         Longer_array = 1\n",
    "#     return idx, Longer_array\n",
    "#\n",
    "# for i in clusters_2[0].indices:\n",
    "#\n",
    "#     index, Longer_Array_Test_Array = find_nearest(Test_Array, streams_work[i]) # 轨迹对齐\n",
    "#\n",
    "#     if Longer_Array_Test_Array:\n",
    "#         for center_point , stream_point in zip(Test_Array[index:,:],streams_work[i]):\n",
    "#             # geopy.distance.distance([p[0][0], p[0][1]], [p[1][0], p[1][1]]).kilometers  for p in list(zip(clusters_2.centroids[0], stream))\n",
    "#             print(\"HERE!!!\")\n",
    "#             print(center_point , stream_point)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-df77fae2",
   "language": "python",
   "display_name": "PyCharm (object-CXR)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}